# -*- coding: utf-8 -*-
"""Analisando sentimentos boratrocar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fKpujUERPXnTbcd2sO_wQ33ydErZG3m0

# Analise exploratória
"""

import pandas as pd

# Leitura do arquivo com separador correto
csv_path = "/content/base comentarios(in) (4).csv"
df = pd.read_csv(csv_path, sep=';')

# Mostrar as 5 primeiras linhas
print("### 5 primeiras linhas ###")
print(df.head())

# Informações gerais (colunas, tipos de dados, não nulos)
print("\n### Informações gerais do DataFrame ###")
print(df.info())

# Estatísticas descritivas para colunas numéricas
print("\n### Estatísticas descritivas (numéricas) ###")
print(df.describe())

# Estatísticas para colunas texto (comentarios)
print("\n### Estatísticas para colunas texto ###")
print(df['comentario'].describe())

# Contagem de valores nulos por coluna
print("\n### Contagem de valores nulos ###")
print(df.isnull().sum())

# Verificar se existem valores duplicados
print("\n### Quantidade de linhas duplicadas ###")
print(df.duplicated().sum())

"""# Rotulando sentimentos, com VADER (LEiA)"""

#repositório adpatado para ptBr  LeIA
!git clone https://github.com/rafjaa/LeIA.git

# ↓ Executar só uma vez
nltk.download('stopwords')
nltk.download('punkt')

import pandas as pd
import re
import nltk
import unicodedata
from nltk.corpus import stopwords
from nltk.tokenize.toktok import ToktokTokenizer
import sys

# Configurar tokenizer e nltk
tokenizer = ToktokTokenizer()
nltk.download('stopwords')
nltk.download('punkt')


csv_text = ("/content/base comentarios(in) (4).csv")

df = pd.read_csv(csv_text, sep=';')
print(df.columns.tolist())


# Importar SentimentIntensityAnalyzer do LeIA
sys.path.append("/content/LeIA")
from leia import SentimentIntensityAnalyzer
analisador = SentimentIntensityAnalyzer()

# Função para limpar texto
def limpar_texto(texto):
    if pd.isna(texto):
        return ''
    texto = str(texto)
    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')
    texto = texto.lower()
    texto = re.sub(r'[^a-zA-Z\s]', '', texto)
    palavras = tokenizer.tokenize(texto)
    stop_words = set(stopwords.words('portuguese')) - {'legal', 'bom', 'ruim', 'excelente', 'horrivel', 'horrível'}
    palavras_filtradas = [p for p in palavras if p not in stop_words and len(p) > 1]
    return ' '.join(palavras_filtradas)

# Função para analisar sentimento
def analisar_sentimento(texto):
    score = analisador.polarity_scores(str(texto))['compound']
    if score > 0.05:
        return "Positivo"
    elif score < -0.05:
        return "Negativo"
    else:
        return "Neutro"

# Limpar comentários (opcional para a análise, mas bom para inspecionar)
df['comentario_limpo'] = df['comentario'].apply(limpar_texto)

# Criar coluna de sentimentos
df['sentimentos'] = df['comentario'].apply(analisar_sentimento)

# Salvar resultado em CSV
df.to_csv("/content/comentarios_com_sentimentos.csv", index=False)

print("Arquivo salvo: /content/comentarios_com_sentimentos.csv")
print(df[['id_comentario', 'comentario', 'sentimentos']].head(10))

"""# Treinando Rede neural com CNN"""

# Instala TensorFlow e nltk
!pip install tensorflow nltk unidecode

import numpy as np
import pandas as pd
import unidecode
import nltk
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Baixar recursos do NLTK
nltk.download('stopwords')
nltk.download('rslp')

#arquivo a ser analizado
df = pd.read_csv('/content/comentarios_com_sentimentos.csv')

# Preprocessamento
stopwords_pt = stopwords.words('portuguese')
stemmer = RSLPStemmer()

def preprocessar(texto):
    texto = str(texto).lower()
    texto = unidecode.unidecode(texto)
    palavras = texto.split()
    palavras = [stemmer.stem(p) for p in palavras if p not in stopwords_pt]
    return " ".join(palavras)

df['comentario_limpo'] = df['comentario'].apply(preprocessar)

# Converter rótulos de sentimento para números
le = LabelEncoder()
df['label'] = le.fit_transform(df['sentimentos'])  # ex: Negativo=0, Neutro=1, Positivo=2

# Tokenização
max_words = 2000
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(df['comentario_limpo'])
sequences = tokenizer.texts_to_sequences(df['comentario_limpo'])

# Padding
max_len = 30
X = pad_sequences(sequences, maxlen=max_len, padding='post')

# One-hot encoding dos rótulos
num_classes = len(le.classes_)
y = to_categorical(df['label'], num_classes=num_classes)

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Modelo CNN para classificação múltipla
model = Sequential([
    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),
    Conv1D(64, kernel_size=3, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Treinamento
model.fit(X_train, y_train, epochs=15, batch_size=4, validation_split=0.2)

# Avaliação
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\n✅ Acurácia no teste: {accuracy:.2f}")

# Exemplo de previsão com novo comentário
novo_texto = "Adorei o livro, muito bom!"
novo_texto_limpo = preprocessar(novo_texto)
seq = tokenizer.texts_to_sequences([novo_texto_limpo])
seq_pad = pad_sequences(seq, maxlen=max_len, padding='post')
pred = model.predict(seq_pad)
classe_pred = le.inverse_transform([np.argmax(pred)])
print(f"Sentimento previsto: {classe_pred[0]}")